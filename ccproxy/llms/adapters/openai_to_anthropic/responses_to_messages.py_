from __future__ import annotations

import json
import re
from collections.abc import AsyncGenerator, AsyncIterator
from typing import Any

from pydantic import BaseModel

from ccproxy.llms.adapters.base import BaseAPIAdapter
from ccproxy.llms.adapters.shared import (
    convert_openai_error_to_anthropic,
    convert_openai_response_usage_to_anthropic_usage,
)
from ccproxy.llms.adapters.anthropic_to_openai.helpers import (
    convert_anthropic_usage_to_openai_response_usage,
)
from ccproxy.llms.anthropic import models as anthropic_models
from ccproxy.llms.anthropic.models import MessageResponse as AnthropicMessageResponse
from ccproxy.llms.openai import models as openai_models
from ccproxy.llms.openai.models import ResponseRequest


THINKING_PATTERN = re.compile(
    r"<thinking(?:\s+signature=\"([^\"]*)\")?>(.*?)</thinking>",
    re.DOTALL,
)


class OpenAIResponsesToAnthropicAdapter(
    BaseAPIAdapter[
        BaseModel,
        BaseModel,
        openai_models.AnyStreamEvent,
    ]
):
    """OpenAI Responses â†’ Anthropic Messages adapter (non-streaming + streaming subset)."""

    def __init__(self) -> None:
        super().__init__(name="openai_responses_to_anthropic")

    async def adapt_request(self, request: BaseModel) -> BaseModel:
        if not isinstance(request, ResponseRequest):
            raise ValueError(f"Expected ResponseRequest, got {type(request)}")

        return convert_openai_response_request_to_anthropic(request)

    async def adapt_response(self, response: BaseModel) -> AnthropicMessageResponse:
        if not isinstance(response, openai_models.ResponseObject):
            raise ValueError(f"Expected ResponseObject, got {type(response)}")

        return convert_openai_response_to_anthropic_message(response)

    def adapt_stream(
        self,
        stream: AsyncIterator[openai_models.AnyStreamEvent],
    ) -> AsyncGenerator[anthropic_models.MessageStreamEvent, None]:
        return self._convert_stream(stream)

    async def adapt_error(self, error: BaseModel) -> BaseModel:
        return convert_openai_error_to_anthropic(error)


__all__ = [
    "OpenAIResponsesToAnthropicAdapter",
    "convert_openai_response_to_anthropic_message",
]


def convert_openai_response_request_to_anthropic(
    request: ResponseRequest,
) -> anthropic_models.CreateMessageRequest:
    """Minimal mapping: map model, instructions->system, input text to user message."""
    system = request.instructions
    user_text = None
    if isinstance(request.input, str):
        user_text = request.input
    else:
        # find first input_text content
        for item in request.input or []:
            contents = getattr(item, "content", [])
            for part in contents or []:
                if getattr(part, "type", None) in {"input_text", "text"}:
                    txt = getattr(part, "text", None)
                    if isinstance(txt, str):
                        user_text = txt
                        break
            if user_text:
                break

    messages: list[dict[str, Any]] = []
    if user_text is not None:
        messages.append({"role": "user", "content": user_text})

    payload: dict[str, Any] = {
        "model": request.model,
        "messages": messages,
    }
    if system:
        payload["system"] = system
    if request.max_output_tokens is not None:
        payload["max_tokens"] = request.max_output_tokens

    return anthropic_models.CreateMessageRequest.model_validate(payload)


class OpenAIResponsesRequestToAnthropicMessagesAdapter(
    BaseAPIAdapter[openai_models.ResponseRequest, anthropic_models.MessageResponse, openai_models.AnyStreamEvent]
):
    def __init__(self) -> None:
        super().__init__(name="openai_responses_request_to_anthropic_messages")

    async def adapt_request(self, request: BaseModel) -> BaseModel:
        if not isinstance(request, openai_models.ResponseRequest):
            raise ValueError(f"Expected ResponseRequest, got {type(request)}")
        return convert_openai_response_request_to_anthropic(request)

    async def adapt_response(self, response: BaseModel) -> BaseModel:  # pragma: no cover
        raise NotImplementedError

    def adapt_stream(self, stream):  # pragma: no cover
        raise NotImplementedError
