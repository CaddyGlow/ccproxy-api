from __future__ import annotations

from collections.abc import AsyncGenerator, AsyncIterator
from typing import Any

from pydantic import BaseModel

from ccproxy.llms.adapters.openai_to_openai.helpers import (
    convert_openai_completion_usage_to_openai_response_usage,
    convert_openai_response_usage_to_openai_completion_usage,
)
from ccproxy.llms.openai import models as openai_models
from ccproxy.llms.adapters.base import BaseAPIAdapter
from ccproxy.llms.openai.models import (
    ChatCompletionChunk,
    ChatCompletionRequest,
    ChatCompletionResponse,
    ResponseObject,
    ResponseRequest,
)


class OpenAIChatToOpenAIResponsesAdapter(
    BaseAPIAdapter[
        ChatCompletionRequest,
        ResponseObject,
        BaseModel,
    ]
):
    """OpenAI Chat → OpenAI Responses request adapter (minimal).

    Implemented
    - model: passthrough
    - max_completion_tokens/max_tokens → `max_output_tokens`
    - messages: maps the last `user` message text to a single Responses `input` message

    TODO
    - Map all conversation turns to multi-item `input` if needed
    - Map richer contents (images, tools) to Responses-supported forms
    - Pass through response_format as-is if present on Chat (hybrid flows)
    """

    def __init__(self) -> None:
        super().__init__(name="openai_chat_to_openai_responses")

    # Typed methods - delegate to dict implementation for now
    async def adapt_request(self, request: ChatCompletionRequest) -> ResponseObject:
        if not isinstance(request, ChatCompletionRequest):
            raise ValueError(f"Expected ChatCompletionRequest, got {type(request)}")
        return await self._convert_request(request)

    async def adapt_response(self, response: ChatCompletionResponse) -> ResponseObject:
        if not isinstance(response, ResponseObject):
            raise ValueError(f"Expected ResponseObject, got {type(response)}")

        return convert_openai_response_to_chat(response)

    def adapt_stream(
        self, stream: AsyncIterator[openai_models.ChatCompletionChunk]
    ) -> AsyncGenerator[openai_models.AnyStreamEvent, None]:
        raise NotImplementedError("Streaming not implemented for this adapter")

    async def adapt_error(self, error: BaseModel) -> BaseModel:
        return error  # Pass through

    async def _convert_request(self, request: ChatCompletionRequest) -> ResponseRequest:
        """Convert ChatCompletionRequest to ResponseRequest using typed models."""
        model = request.model
        max_out = request.max_completion_tokens or request.max_tokens

        # Find the last user message
        user_text: str | None = None
        for msg in reversed(request.messages or []):
            if msg.role == "user":
                content = msg.content
                if isinstance(content, list):
                    texts = [
                        part.text
                        for part in content
                        if hasattr(part, "type")
                        and part.type == "text"
                        and hasattr(part, "text")
                    ]
                    user_text = " ".join([t for t in texts if t])
                else:
                    user_text = content
                break

        input_data = []
        if user_text:
            input_msg = {
                "type": "message",
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": user_text,
                    }
                ],
            }
            input_data = [input_msg]

        payload_data: dict[str, Any] = {
            "model": model,
        }
        if max_out is not None:
            payload_data["max_output_tokens"] = int(max_out)
        if input_data:
            payload_data["input"] = input_data

        # Structured outputs: map Chat response_format to Responses text.format
        resp_fmt = request.response_format
        if resp_fmt is not None:
            if resp_fmt.type == "text":
                payload_data["text"] = {"format": {"type": "text"}}
            elif resp_fmt.type == "json_object":
                payload_data["text"] = {"format": {"type": "json_object"}}
            elif resp_fmt.type == "json_schema" and hasattr(resp_fmt, "json_schema"):
                js = resp_fmt.json_schema
                # Pass through name/schema/strict if provided
                fmt = {"type": "json_schema"}
                if js is not None:
                    js_dict = js.model_dump() if hasattr(js, "model_dump") else js
                    if js_dict is not None:
                        fmt.update(
                            {
                                k: v
                                for k, v in js_dict.items()
                                if k
                                in {"name", "schema", "strict", "$defs", "description"}
                            }
                        )
                payload_data["text"] = {"format": fmt}

        if request.tools:
            payload_data["tools"] = [
                tool.model_dump() if hasattr(tool, "model_dump") else tool
                for tool in request.tools
            ]

        return ResponseRequest.model_validate(payload_data)


def convert_openai_response_to_chat(
    response: openai_models.ResponseObject,
) -> openai_models.ChatCompletionResponse:
    """Convert an OpenAI ResponseObject to a ChatCompletionResponse."""
    # Find first message output and aggregate output_text parts
    text_content = ""
    for item in response.output or []:
        if hasattr(item, "type") and item.type == "message":
            parts: list[str] = []
            for part in getattr(item, "content", []):
                if hasattr(part, "type") and part.type == "output_text":
                    if hasattr(part, "text") and isinstance(part.text, str):
                        parts.append(part.text)
                elif isinstance(part, dict) and part.get("type") == "output_text":
                    text = part.get("text")
                    if isinstance(text, str):
                        parts.append(text)
            text_content = "".join(parts)
            break

    usage = None
    if response.usage:
        usage = convert_openai_response_usage_to_openai_completion_usage(response.usage)

    return openai_models.ChatCompletionResponse(
        id=response.id or "chatcmpl-resp",
        choices=[
            openai_models.Choice(
                index=0,
                message=openai_models.ResponseMessage(
                    role="assistant", content=text_content
                ),
                finish_reason="stop",
            )
        ],
        created=0,
        model=response.model or "",
        object="chat.completion",
        usage=usage
        or openai_models.CompletionUsage(
            prompt_tokens=0, completion_tokens=0, total_tokens=0
        ),
    )
