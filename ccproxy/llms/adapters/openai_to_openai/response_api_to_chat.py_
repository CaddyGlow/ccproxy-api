from __future__ import annotations

import time
from collections.abc import AsyncGenerator, AsyncIterator
from typing import Any

from pydantic import BaseModel

from ccproxy.llms.adapters.base import BaseAPIAdapter
from ccproxy.llms.adapters.openai_to_openai.helpers import (
    convert_openai_completion_usage_to_openai_response_usage,
)
from ccproxy.llms.openai import models as openai_models
from ccproxy.llms.openai.models import (
    AnyStreamEvent,
    ChatCompletionChunk,
    ChatCompletionRequest,
    ChatCompletionResponse,
    InputTokensDetails,
    MessageOutput,
    OutputTextContent,
    OutputTokensDetails,
    ResponseObject,
    ResponseRequest,
    ResponseUsage,
)


class ResponseAPIToOpenAIChatAdapter(
    BaseAPIAdapter[
        openai_models.ResponseRequest,
        openai_models.ChatCompletionResponse,
        openai_models.AnyStreamEvent,
    ]
):
    """Convert Response API payloads to OpenAI Chat Completions."""

    def __init__(self) -> None:
        super().__init__(name="response_api_to_openai_chat")

    async def adapt_request(self, request: BaseModel) -> ChatCompletionRequest:
        if not isinstance(request, ResponseRequest):
            raise ValueError(f"Expected ResponseRequest, got {type(request)}")

        return await self._convert_request(request)

    async def adapt_response(self, response: BaseModel) -> ResponseObject:
        if not isinstance(response, ChatCompletionResponse):
            raise ValueError(f"Expected ChatCompletionResponse, got {type(response)}")

        return await self._convert_response(response)

    def adapt_stream(
        self, stream: AsyncIterator[AnyStreamEvent]
    ) -> AsyncGenerator[ChatCompletionChunk, None]:
        """Convert Response API stream events to OpenAI ChatCompletion chunks."""
        return response_stream_to_chat_chunks(stream)

    async def adapt_error(self, error: BaseModel) -> BaseModel:
        # Chat Completions and Response API currently share the same error envelope
        return error
