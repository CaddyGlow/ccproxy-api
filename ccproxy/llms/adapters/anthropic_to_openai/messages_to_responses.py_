from __future__ import annotations

from collections.abc import AsyncGenerator, AsyncIterator
from typing import Any, cast

from pydantic import BaseModel

from ccproxy.llms.adapters.base import BaseAPIAdapter
from ccproxy.llms.adapters.anthropic_to_openai.helpers import (
    convert_anthropic_error_to_openai,
)
from ccproxy.llms.adapters.anthropic_to_openai.helpers import (
    convert_anthropic_usage_to_openai_response_usage,
)
from ccproxy.llms.anthropic import models as anthropic_models
from ccproxy.llms.openai import models as openai_models


ResponseStreamEvent = (
    openai_models.ResponseCreatedEvent
    | openai_models.ResponseInProgressEvent
    | openai_models.ResponseCompletedEvent
    | openai_models.ResponseOutputTextDeltaEvent
    | openai_models.ResponseFunctionCallArgumentsDoneEvent
    | openai_models.ResponseRefusalDoneEvent
)


class AnthropicMessagesToOpenAIResponsesAdapter(
    BaseAPIAdapter[
        anthropic_models.CreateMessageRequest,
        anthropic_models.MessageResponse,
        anthropic_models.MessageStreamEvent,
    ]
):
    """Anthropic Messages ↔ OpenAI Responses adapter (request + response subset).

    Implemented
    - adapt_request: Anthropic → OpenAI Responses request
      - Maps `model`, `stream`, `max_tokens` → `max_output_tokens`
      - Maps last user message text into a single Responses `input` message
      - Maps custom tools → function tools
      - Maps tool_choice (auto/any/tool/none) and `parallel_tool_calls`
      - Maps `system` into `instructions` when present
    - adapt_response: Anthropic → OpenAI Responses response
      - Serializes `thinking` blocks into a single OutputTextContent using
        <thinking signature="…">…</thinking> XML followed by visible text
      - Maps ToolUseBlock into content dicts with type "tool_use"
      - Maps usage fields into OpenAI ResponseUsage

    TODO
    - Expand request mapping to include multiple turns and multimodal content
    - Include richer content types beyond text and tool use
    - Consider mapping stop reasons to status/incomplete details
    """

    def __init__(self) -> None:
        super().__init__(name="anthropic_messages_to_openai_responses")

    # Strongly-typed methods
    async def adapt_request(self, request: BaseModel) -> BaseModel:
        """Convert Anthropic CreateMessageRequest to OpenAI ResponseRequest."""
        if not isinstance(request, anthropic_models.CreateMessageRequest):
            raise ValueError(f"Expected CreateMessageRequest, got {type(request)}")

        return await self._convert_request(request)

    async def adapt_response(self, response: BaseModel) -> BaseModel:
        """Convert Anthropic MessageResponse to OpenAI ResponseObject."""
        if not isinstance(response, anthropic_models.MessageResponse):
            raise ValueError(f"Expected MessageResponse, got {type(response)}")

        return await self._convert_response(response)

    def adapt_stream(
        self, stream: AsyncIterator[anthropic_models.MessageStreamEvent]
    ) -> AsyncGenerator[ResponseStreamEvent, None]:
        """Convert Anthropic MessageStreamEvent stream to OpenAI Response stream events."""
        return self._convert_stream(stream)

    async def adapt_error(self, error: BaseModel) -> BaseModel:
        """Convert Anthropic error payloads to the OpenAI envelope."""
        return convert_anthropic_error_to_openai(error)

    # Implementation methods

    async def _convert_response(
        self, response: anthropic_models.MessageResponse
    ) -> openai_models.ResponseObject:
        """Convert Anthropic MessageResponse to OpenAI ResponseObject using typed models."""
        return convert_anthropic_message_to_response_object(response)

    async def _adapt_response_dict_impl(
        self, response: dict[str, Any]
    ) -> dict[str, Any]:
        """Implementation moved from adapt_response - works with dicts."""
        anthropic_response = anthropic_models.MessageResponse.model_validate(response)
        return convert_anthropic_message_to_response_object(
            anthropic_response
        ).model_dump()


async def convert__anthropic_messsage_to_openai_response__stream(
    stream: AsyncIterator[anthropic_models.MessageStreamEvent],
) -> AsyncGenerator[ResponseStreamEvent, None]:
    item_id = "msg_stream"
    output_index = 0
    content_index = 0
    model_id = ""
    response_id = ""
    sequence_counter = 0

    async for evt in stream:
        if not hasattr(evt, "type"):
            continue

        sequence_counter += 1

        if evt.type == "message_start":
            model_id = evt.message.model or ""
            response_id = evt.message.id or ""
            yield openai_models.ResponseCreatedEvent(
                type="response.created",
                sequence_number=sequence_counter,
                response=openai_models.ResponseObject(
                    id=response_id,
                    object="response",
                    created_at=0,
                    status="in_progress",
                    model=model_id,
                    output=[],
                    parallel_tool_calls=False,
                ),
            )

            # Handle pre-filled content like thinking blocks
            for block in evt.message.content:
                if block.type == "thinking":
                    sequence_counter += 1
                    thinking = block.thinking or ""
                    signature = block.signature
                    sig_attr = f' signature="{signature}"' if signature else ""
                    thinking_xml = f"<thinking{sig_attr}>{thinking}</thinking>"
                    yield openai_models.ResponseOutputTextDeltaEvent(
                        type="response.output_text.delta",
                        sequence_number=sequence_counter,
                        item_id=item_id,
                        output_index=output_index,
                        content_index=content_index,
                        delta=thinking_xml,
                    )

        elif evt.type == "content_block_start":
            if evt.content_block.type == "tool_use":
                tool_input = evt.content_block.input or {}
                try:
                    import json

                    args_str = json.dumps(tool_input, separators=(",", ":"))
                except Exception:
                    args_str = str(tool_input)

                yield openai_models.ResponseFunctionCallArgumentsDoneEvent(
                    type="response.function_call_arguments.done",
                    sequence_number=sequence_counter,
                    item_id=item_id,
                    output_index=output_index,
                    arguments=args_str,
                )

        elif evt.type == "content_block_delta":
            text = evt.delta.text
            if text:
                yield openai_models.ResponseOutputTextDeltaEvent(
                    type="response.output_text.delta",
                    sequence_number=sequence_counter,
                    item_id=item_id,
                    output_index=output_index,
                    content_index=content_index,
                    delta=text,
                )

        elif evt.type == "message_delta":
            yield openai_models.ResponseInProgressEvent(
                type="response.in_progress",
                sequence_number=sequence_counter,
                response=openai_models.ResponseObject(
                    id=response_id,
                    object="response",
                    created_at=0,
                    status="in_progress",
                    model=model_id,
                    output=[],
                    parallel_tool_calls=False,
                    usage=cast(
                        openai_models.ResponseUsage,
                        convert_anthropic_usage_to_openai_response_usage(evt.usage),
                    ),
                ),
            )
            if evt.delta.stop_reason == "refusal":
                sequence_counter += 1
                yield openai_models.ResponseRefusalDoneEvent(
                    type="response.refusal.done",
                    sequence_number=sequence_counter,
                    item_id=item_id,
                    output_index=output_index,
                    content_index=content_index,
                    refusal="refused",
                )

        elif evt.type == "message_stop":
            yield openai_models.ResponseCompletedEvent(
                type="response.completed",
                sequence_number=sequence_counter,
                response=openai_models.ResponseObject(
                    id=response_id,
                    object="response",
                    created_at=0,
                    status="completed",
                    model=model_id,
                    output=[],
                    parallel_tool_calls=False,
                ),
            )
            break


def convert__anthropic_messsage_to_openai_response__request(
    request: anthropic_models.CreateMessageRequest,
) -> openai_models.ResponseRequest:
    """Convert Anthropic CreateMessageRequest to OpenAI ResponseRequest using typed models."""
    # Build OpenAI Responses request payload
    payload_data: dict[str, Any] = {
        "model": request.model,
    }

    if request.max_tokens is not None:
        payload_data["max_output_tokens"] = int(request.max_tokens)
    if request.stream:
        payload_data["stream"] = True

    # Map system to instructions if present
    if request.system:
        if isinstance(request.system, str):
            payload_data["instructions"] = request.system
        else:
            payload_data["instructions"] = "".join(
                block.text for block in request.system
            )

    # Map last user message text to Responses input
    last_user_text: str | None = None
    for msg in reversed(request.messages):
        if msg.role == "user":
            if isinstance(msg.content, str):
                last_user_text = msg.content
            elif isinstance(msg.content, list):
                texts: list[str] = []
                for block in msg.content:
                    # Support raw dicts and models
                    if isinstance(block, dict):
                        if block.get("type") == "text" and isinstance(
                            block.get("text"), str
                        ):
                            texts.append(block.get("text") or "")
                    else:
                        # Type guard for TextBlock
                        if (
                            getattr(block, "type", None) == "text"
                            and hasattr(block, "text")
                            and isinstance(getattr(block, "text", None), str)
                        ):
                            texts.append(block.text or "")
                if texts:
                    last_user_text = " ".join(texts)
            break

    # Always provide an input field matching ResponseRequest schema
    if last_user_text:
        payload_data["input"] = [
            {
                "type": "message",
                "role": "user",
                "content": [
                    {"type": "input_text", "text": last_user_text},
                ],
            }
        ]
    else:
        # Provide an empty input list if no user text detected to satisfy schema
        payload_data["input"] = []

    # Tools mapping (custom tools -> function tools)
    if request.tools:
        tools: list[dict[str, Any]] = []
        for tool in request.tools:
            if isinstance(tool, anthropic_models.Tool):
                tools.append(
                    {
                        "type": "function",
                        "function": {
                            "name": tool.name,
                            "description": tool.description,
                            "parameters": tool.input_schema,
                        },
                    }
                )
        if tools:
            payload_data["tools"] = tools

    # tool_choice mapping (+ parallel control)
    tc = request.tool_choice
    if tc is not None:
        tc_type = getattr(tc, "type", None)
        if tc_type == "none":
            payload_data["tool_choice"] = "none"
        elif tc_type == "auto":
            payload_data["tool_choice"] = "auto"
        elif tc_type == "any":
            payload_data["tool_choice"] = "required"
        elif tc_type == "tool":
            name = getattr(tc, "name", None)
            if name:
                payload_data["tool_choice"] = {
                    "type": "function",
                    "function": {"name": name},
                }
        disable_parallel = getattr(tc, "disable_parallel_tool_use", None)
        if isinstance(disable_parallel, bool):
            payload_data["parallel_tool_calls"] = not disable_parallel

    # Validate
    return openai_models.ResponseRequest.model_validate(payload_data)


async def convert__anthropic_message_to_openai_chat__stream(
    stream: AsyncIterator[MessageStreamEvent],
) -> AsyncGenerator[ChatCompletionChunk, None]:
    """Convert Anthropic stream to OpenAI stream using typed models."""

    async def generator() -> AsyncGenerator[ChatCompletionChunk, None]:
        model_id = ""
        finish_reason: FinishReason = "stop"
        usage_prompt = 0
        usage_completion = 0

        async for evt in stream:
            if not hasattr(evt, "type"):
                continue

            if evt.type == "message_start":
                model_id = evt.message.model or ""
            elif evt.type == "content_block_delta":
                text = evt.delta.text
                if text:
                    yield ChatCompletionChunk(
                        id="chatcmpl-stream",
                        object="chat.completion.chunk",
                        created=0,
                        model=model_id,
                        choices=[
                            openai_models.StreamingChoice(
                                index=0,
                                delta=openai_models.DeltaMessage(
                                    role="assistant", content=text
                                ),
                                finish_reason=None,
                            )
                        ],
                    )
            elif evt.type == "message_delta":
                if evt.delta.stop_reason:
                    finish_reason = cast(
                        FinishReason,
                        ANTHROPIC_TO_OPENAI_FINISH_REASON.get(
                            evt.delta.stop_reason, "stop"
                        ),
                    )
                usage_prompt = evt.usage.input_tokens
                usage_completion = evt.usage.output_tokens
            elif evt.type == "message_stop":
                usage = None
                if usage_prompt or usage_completion:
                    usage = openai_models.CompletionUsage(
                        prompt_tokens=usage_prompt,
                        completion_tokens=usage_completion,
                        total_tokens=usage_prompt + usage_completion,
                    )
                yield ChatCompletionChunk(
                    id="chatcmpl-stream",
                    object="chat.completion.chunk",
                    created=0,
                    model=model_id,
                    choices=[
                        openai_models.StreamingChoice(
                            index=0,
                            delta=openai_models.DeltaMessage(),
                            finish_reason=finish_reason,
                        )
                    ],
                    usage=usage,
                )
                break

    return generator()


def convert__anthropic_message_to_openai_response_object(
    response: anthropic_models.MessageResponse,
) -> openai_models.ResponseObject:
    """Convert Anthropic MessageResponse to an OpenAI ResponseObject."""
    text_parts: list[str] = []
    tool_contents: list[dict[str, Any]] = []
    for block in response.content:
        block_type = getattr(block, "type", None)
        if block_type == "text":
            text_parts.append(getattr(block, "text", ""))
        elif block_type == "thinking":
            thinking = getattr(block, "thinking", None) or ""
            signature = getattr(block, "signature", None)
            sig_attr = (
                f' signature="{signature}"'
                if isinstance(signature, str) and signature
                else ""
            )
            text_parts.append(f"<thinking{sig_attr}>{thinking}</thinking>")
        elif block_type == "tool_use":
            tool_contents.append(
                {
                    "type": "tool_use",
                    "id": getattr(block, "id", "tool_1"),
                    "name": getattr(block, "name", "function"),
                    "arguments": getattr(block, "input", {}) or {},
                }
            )

    message_content: list[dict[str, Any]] = []
    if text_parts:
        message_content.append(
            openai_models.OutputTextContent(
                type="output_text",
                text="".join(text_parts),
            ).model_dump()
        )
    message_content.extend(tool_contents)

    usage_model = None
    if response.usage is not None:
        usage_model = cast(
            openai_models.ResponseUsage,
            convert_anthropic_usage_to_openai_response_usage(response.usage),
        )

    return openai_models.ResponseObject(
        id=response.id,
        object="response",
        created_at=0,
        status="completed",
        model=response.model,
        output=[
            openai_models.MessageOutput(
                type="message",
                id=f"{response.id}_msg_0",
                status="completed",
                role="assistant",
                content=message_content,  # type: ignore[arg-type]
            )
        ],
        parallel_tool_calls=False,
        usage=usage_model,
    )
